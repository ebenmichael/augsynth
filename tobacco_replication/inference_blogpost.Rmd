---
title: Comparing inference approaches for the synthetic control method with the `augsynth`
  package
author: "Miratrix & Delaney"
date: "2024-11-16"
output:
  pdf_document: default
  html_document: default
subtitle: Also replicating Abadie et al. (2010)
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(
    collapse = FALSE,
    message = FALSE,
    warning = FALSE,
    fig.align = 'center',
    comment = "#>"
)

## Install Synth if not already installed
# install.packages("Synth")

library(kableExtra)
library(magrittr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(tidyr)
library(rlang)

library(foreign)
library(graphics)
library(Synth)
library(augsynth)

## For the Abadie plots
replication_theme <- theme_bw(base_family = "Times New Roman") + 
    theme(panel.grid = element_blank(), 
          legend.background = element_rect(color = 'black'),
          legend.spacing.y = unit(0.0, 'pt'),
          legend.box.margin = margin(t = 5, r = 5, unit = 'pt'),
          legend.position = c(0.81, 0.9))

set.seed(1234)
```


There are many competing approaches to inference in the Synthetic Control world, and these approaches rely on a variety of different sets of assumptions that researchers can get quite agitated about.
In this post, we are going to compare some of these approaches by re-analyzing the famous "Tobacco example" originally laid out in [Abadie et al. (2010)](https://web.stanford.edu/~jhain/Paper/JASA2010.pdf) using some cool new features we recently added to the `augsynth` package.
This post also shows off our `augsynth` package, and in particular demonstrates how fitting and exploring synthetic control models can be easily done with only a few lines of code.

As a tiny bit of background, the synthetic control approach has two major steps.
The first step is to generate the synthetic control.
Intuitively, this consists of using some sort of algorithm to reweight a set of possible "donor" units so that their weighted average "looks like" the target treated unit, usually with respect to the treated unit's pre-treatment outcome trajectory and possibly some other baseline covariates besides.
There are an ever-increasing variety of ways one might do this, which we will discuss a bit more below.
Once you have your synthetic control, you estimate treatment impacts as the difference in the treated outcome and the synthetic control outcome for each post-treatment time period.

With synthetic control and consequent treatment impacts in hand, the second step is to figure out how to conduct inference: in the first step we deliberately manufactured a "synthetic" unit to look like our treated unit, and now we want to know if the difference in their behaviors post-treatment could be due to random chance, or something structural like a treatment effect.
Inference is a thorny problem since there is no easily well defined model or source of randomness to be seen!


## A note on different packages and approaches

In terms of R packages for fitting a synthetic control, we focus on two contenders.
The first is the `Synth` package, which has the algorithm and approach outlined in the seminal Abadie et al. (2010).
The second is the newer `augsynth` package, that offers a variety of different methods for building a synthetic comparison, and in fact _does not_ offer the exact same fitting approach used in `Synth`.
Nevertheless, we can fairly closely mimic the spirit of the original `Synth` with `augsynth`, as measured by getting fairly close to the same results shown in Figures 2--7 in the 2010 paper.
The `augsynth` package also offers a wider array of inference approaches which, as of the latest update, includes the permutation approach advocated for by the original Synth papers.
Before diving into inference, we first discuss how Synth and augsynth are different in how they build the synthetic control.

To use the `Synth` package, you specify a set of features (including a subset of pre-treatment outcome measures) you want to match on, and a separate set of pre-treatment "test" outcomes to assess performance on, and then the package will find the best weighting of the match features that produces the closest matched synthetic control, as measured by the specified pre-treatment "test" outcomes.
The package does this with a nested optimization of, for a given set of variable weights, finding the besdt  weighting of the donor units, and then finding the best variable weights that gives the "best of the best" synthetic control.
This iterative procedure results in two sets of weights, the "V" weights, which weight the possible match variables, and the donor weights, which weight the donor units to give the final synthetic control.
This approach makes it easy to include both lagged outcomes *and* auxiliary covariates, as the V weights will essentially rescale covariates to optimize any tradeoffs between which covariates to prioritize when building a match.

The `augsynth` package, by contrast, directly finds unit weights to minimize the mean squared difference between the synthetic control and the treated unit across all pre-treatment outcome periods and any additional covariates.
The scaling of match covariates can be done as a pre-processing step, but is not done within the package.
Instead of focusing on V weights, the `augsynth` package then allows for balance correction via a (usually ridge) regression step.
This second step is the "augmentation" part, and it can help regularize weights so, in the case of rich donor unit contexts, more donor units can be included in the synthetic match.
That said, the `augsynth` package can also just match on pre-treatment trends and not do any further adjustment, which is closer to the original `Synth` approach (but without being able to up- or down-weight some covariates or pre-treatment outcomes over others).

A more careful replication of the original Synth analysis with augsynth can be found in the new vignette in the package.

<!--We primarily focus on the 
The Rmd version of this notebook includes `Synth` code to generate Abadie et al.'s results with a high degree of fidelity (with the resulting `Synth`-based plots shown adjacent to analogous output from `augsynth` models).
-->


```{r import tobacco data, include=FALSE}
# Get working data CA and 38 control states 
Wk.data <- read.dta(here::here("./tobacco_replication/data/smoking_wkdata_39.dta"))

tobacco <- Wk.data %>%
    mutate(treated = ifelse((name == 'California') &
                                (year > 1988), 1, 0),
           state = name, # rename something meaningful
           state_index = index) %>%
    select(cigsalepcap, treated, state, year, retprice, xxincome, K1_r_15_24, xxbeer)

tobacco_70 <- tobacco %>% filter(year >= 1970) # subset data to 1970 forward to avoid issues due to missingess of outcomes
```


## Obtaining our synthetic control

The `augsynth` package divides the two steps into two function calls.
To get a synthetic control, we call `augsynth()`. 
To get inference, we call `summary()`.

Here we do our first step of getting our synthetic control.

```{r run tobacco model using augsynth}
syn <- single_augsynth(form = cigsalepcap ~ treated | retprice + xxincome + K1_r_15_24,
                       unit = state,
                       time = year, t_int = 1988,
                       data = tobacco_70 )
syn
```

The above default `augsynth()` call removes bias via a ridge regression adjustment on top of calculating the synthetic control weights.
You can prevent this, making the fitting procedure closer in spirit to the Abadie approach, by including `progfunc = "none"`.
However, in this case, dropping the bias removal gives results that deviate more strongly from the tobacco paper.
We hypothesize that the bias removal is serving the role of the Abadie approach's variable importance weights (the bias removal being a regression that will naturally weight some variables more than others).
In augsynth, without a bias removal step, all the pre-treatment outcomes and covariates are weighted equally (after standardization).

We can plot our result with `plot(plot_type = 'outcomes')`: 

```{r  fig.width=4.5, fig.height=4}
p <- plot(syn, plot_type = 'outcomes raw average') + 
    ggtitle("Replication of Abadie et al. (2010), Figure 2")
p
```

If we do not want the raw average as a comparison line, simply use "outcomes" rather than "outcomes raw average."


## Getting our inference

We get our inference via the `summary()` method, specifying what inference approach to use.
Here we use the permutation approach with impacts standardized by pre-treatment RMSPE:

```{r}
syn_rstat <- summary( syn, inf_type = 'permutation_rstat' )
syn_rstat
```

The inference options include those initially included in the package (`conformal`, `jackknife`, `jackknife+`), and now two that follow Abadie's permutation approach (`permutation` and `permutation_rstat`).
These last two refit the syn model to each donor unit in turn, and use the collection of pseudo-impact estimates as a reference distribution to compare our observed treatment trajectory to.
The `permutation` method uses the raw impact estimates, and `permutation_rstat` is the more recent approach promoted by Abadie of standardizing the gaps by the pre-treatment root mean squared predictive error (RMSPE).

We can plot and compare the confidence bands generated by our four different inference types (the "Jackknife" currently does not provide standard errors, so we omit it here):

```{r plot augsynth models, echo=FALSE, cache=TRUE, fig.width=8, fig.height=4.5}

get_all_inference <- function( syn ) {
    all_results <- bind_rows(
        Conformal = summary(syn, inf_type = 'conformal')$att, 
        #Jackknife = summary(syn, inf_type = 'jackknife')$att, 
        `Jackknife+` = summary(syn, inf_type = 'jackknife+')$att, 
        Permutation = summary(syn, inf_type = 'permutation')$att, 
        `Permutation (rstat)` =  summary(syn, inf_type = 'permutation_rstat')$att,
        .id = 'name') %>% 
        mutate(name = factor(name, 
                             levels = c('Conformal', 'Jackknife', 'Jackknife+',
                                        "Permutation", "Permutation (rstat)"), 
                             ordered = TRUE))
    all_results
}

all_results <- get_all_inference( syn )
ggplot(all_results, aes(x = Time, y = Estimate, color = name)) + 
    geom_hline(yintercept = 0, linetype = 'solid') + 
    geom_vline(xintercept = syn$t_int, linetype = 'dashed') + 
    geom_line() +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = name), 
                alpha = 0.2, size = 0.1) + 
    facet_wrap(. ~ name) + 
    scale_x_continuous(breaks = seq(1970, 2010, 5)) + 
    labs(x = 'Year', y = 'Gap in per-capita cigarette sales\n(in packs)', 
         caption = 'ND replication: augsynth package') +
    scale_color_manual(values = c('red', 'blue', 'darkgreen', 
                                  'purple', 'orange'), 
                       guide = 'none', aesthetics = c('color', 'fill')) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0, color = 'darkseagreen', 
                                      size = 9))
```

There are massive differences!
Conformal and Jackknife+ do not have widening uncertainty as time progresses---this is due to their assumption of, in effect, temporal exchangability.
In my mind this is a nonsensical assumption, and it leads to nonsensical results when extrapolating beyond the time of treatment.
The Jackknife+ seems to be extremely optimistic in terms of its inference, and does not pass face validity, in my mind.
The permutation approaches are more narrow towards the onset of treatment, and get wider as the placebo trajectories diverge--this seems sensible.
Interestingly, the width of these is on the order of the Conformal inference when you average across the span of time.
The RStat approach gives narrower intervals, likely by reducing the effect of outlier donor units that are hard to fit pre-treatment and diverge more post-treatment.
A tension here is whether we are standardizing our test statistic (like a studentized statistic for permutation testing) or artificially discounting the chance that a given unit may diverge markedly from the group.

The critique against permutation inference is a good one: our reference distribution of placebo trajectories is useful only if we think it is reasonable to believe the scale of likely difference between the counterfactual for the treated unit and its synthetic control would indeed be "like" the differences we see for the donor units.
If our treated unit is different from the donor pool (which it likely is---it was the unit that got treatment, after all) then the reference distribution may not a good reference.
For example, if the treatment unit is harder to build a good synthetic control for than the donor units, then how the donor unit trajectories diverge from 0 after the time of treatment may not capture the true randomness (whatever that is) of the treatment unit at all.

All of this said, the reference distribution does give us _some_ sense of what this kind of exercise is likely to do for the kind of data we are looking at.
And, given the other options, it seems much better than any provided alternative.



## Where do the permutation confidence intervals come from?

The confidence regions for the permutation approaches come from measuring the uncertainty of the placebo distribution at each time point.
We can look at the raw placebo trajectories as so:

```{r fig.width=4.5, fig.height=4}
plot(syn_rstat, 
     plot_type = 'placebo',
) + ggtitle("Replication of Abadie et al. (2010), Figure 4")
```

We then use the standard deviation of the trajectories to estimate a rough "Standard Error."
We can also get a table of the placebo trajectories via the `placebo_distribution()` method:

```{r show_perm_inference}
placebo_distribution( syn_rstat )
```

# A second example: Kansas

We can easily replicate the above comparison of the four inference types for the "kansas" data used in the primary vignette of this package (see that vignette and the original paper for more details on these data, and for how we fit the model).^[Full disclosure: we looked across a few different specifications from that vignette, and the inference is not particularly stable. We are trying here to give a reasonable faith choice, and we stand by it being an illustration of _something_ that can naturally happen.  Here we control for covariates, and use the ridge regression adjustment.]

```{r raw_kansas, cache=TRUE, echo=FALSE}
data(kansas)

syn_k <- augsynth( lngdpcapita ~ treated | lngdpcapita + log(revstatecapita) +
                       log(revlocalcapita) + log(avgwklywagecapita) +
                       estabscapita + emplvlcapita,
                   fips, year_qtr, kansas,
                   progfunc = "ridge" )

all_results_k <- get_all_inference( syn_k )

ggplot(all_results_k, aes(x = Time, y = Estimate, color = name)) +
    geom_hline(yintercept = 0, linetype = 'solid') +
    geom_vline(xintercept = syn$t_int, linetype = 'dashed') +
    geom_line() +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = name),
                alpha = 0.2, lwd = 0.1) +
    facet_wrap(. ~ name) +
    scale_x_continuous(breaks = seq(1970, 2010, 5)) +
    labs(x = 'Year', y = 'Impact' ) +
    scale_color_manual(values = c('red', 'blue', 'darkgreen',
                                  'purple', 'orange'),
                       guide = 'none', aesthetics = c('color', 'fill')) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0, color = 'darkseagreen',
                                      size = 9))

```


## Classic synthetic control

We next explore not using the ridge adjustment, which is closer to Abadie's original conception, to see how that impacts the relative uncertainty of the different methods.
This is done by setting `progfunc = "None"`.

```{r}
syn_k_classic <- augsynth( lngdpcapita ~ treated | lngdpcapita + log(revstatecapita) +
                               log(revlocalcapita) + log(avgwklywagecapita) +
                               estabscapita + emplvlcapita,
                           fips, year_qtr, kansas,
                           progfunc = "None" )
```

We plot our envelopes:
```{r, echo=FALSE, cache=TRUE}
all_results_k_classic <- get_all_inference( syn_k_classic )
ggplot(all_results_k_classic, aes(x = Time, y = Estimate, color = name)) + 
    geom_hline(yintercept = 0, linetype = 'solid') + 
    geom_vline(xintercept = syn$t_int, linetype = 'dashed') + 
    geom_line() +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = name), 
                alpha = 0.2, size = 0.1) + 
    facet_wrap(. ~ name) + 
    scale_x_continuous(breaks = seq(1970, 2010, 5)) + 
    labs(x = 'Year', y = 'Impact' ) +
    scale_color_manual(values = c('red', 'blue', 'darkgreen', 
                                  'purple', 'orange'), 
                       guide = 'none', aesthetics = c('color', 'fill')) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0, color = 'darkseagreen', 
                                      size = 9))

```

Here the raw permutation approach gives extremely wide intervals, likely due to extreme outliers in the donor pool. We can check this to see that we indeed have concerns:

```{r fig.width=4.5, fig.height=4}
plot( syn_k_classic, plot_type = 'placebo' ) +
    scale_y_continuous(limits = c(-0.25, 0.5) )
```

If we drop those outliers, we may get a more reasonable set of intervals.
This is easy to do with the `augsynth` package's  `update_augsynth()` method, which will exclude certain donor units and automatically recalculate our synthetic control.
We specify which donor units to drop via the `drop` argument.
If `drop` is passed as a numeric, then all donor units with an RMSPE greater than `drop` times the treated unit's pre-treatment RMSPE will be dropped, following the methods put forth by Abadie.
For example, setting `drop = 2` will exclude any units with a pre-treatment RMSPE twice as large as the treated unit.
The updated augsynth model inherits the formula and data structure of the original model.

We try a factor of 2:

```{r slimmed_kansas}
syn_k_classic_restr <- update_augsynth(syn_k_classic, drop = 2 )
syn_k_classic_restr
```

We have dropped 15 states, and have a somewhat cleaned up plot:

```{r fig.width=4.5, fig.height=4}
plot( syn_k_classic_restr, plot_type = 'placebo' ) +
    scale_y_continuous(limits = c(-0.25, 0.5) )
```

Our final comparative inference plot is then:
```{r, echo=FALSE, cache=TRUE}
all_results_k_classic_restr <- get_all_inference( syn_k_classic_restr )

ggplot(all_results_k_classic_restr, aes(x = Time, y = Estimate, color = name)) +
    geom_hline(yintercept = 0, linetype = 'solid') +
    geom_vline(xintercept = syn$t_int, linetype = 'dashed') +
    geom_line() +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = name),
                alpha = 0.2, lwd = 0.1) +
    facet_wrap(. ~ name) +
    scale_x_continuous(breaks = seq(1970, 2010, 5)) +
    labs(x = 'Year', y = 'Kansas impact' ) +
    scale_color_manual(values = c('red', 'blue', 'darkgreen',
                                  'purple', 'orange'),
                       guide = 'none', aesthetics = c('color', 'fill')) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0, color = 'darkseagreen',
                                      size = 9))

```



# Relative standard errors

For both our examples, we can plot the widths of the confidence intervals.
To get a more rich picture, we will add in different specifications for our inference; in particular, we will use the ridge adjustment and no ridge adjustment.
For all specifications, we continue to adjust with our baseline covariates.

```{r, echo=FALSE, cache=TRUE}

# add in "ridge" for the restricted
syn_k <- augsynth( lngdpcapita ~ treated | lngdpcapita + log(revstatecapita) +
                       log(revlocalcapita) + log(avgwklywagecapita) +
                       estabscapita + emplvlcapita,
                   fips, year_qtr, kansas,
                   progfunc = "ridge" )
all_results_k <- get_all_inference( syn_k )

drp = setdiff( kansas$fips, donor_table( syn_k_classic_restr )$fips )
syn_k_restr <- update_augsynth( syn_k, 
                                drop = as.character(drp) )
all_results_k_restr <- get_all_inference( syn_k_restr )

syn_restr = update_augsynth( syn, drop = 2 )
all_results_restr = get_all_inference( syn_restr )

# Stack tobacco and kansas (ridge)
tt = bind_rows( Tobacco = all_results,
                Tobacco_restr = all_results_restr,
                Kansas = all_results_k,
                Kansas_restr = all_results_k_restr,
                .id = "data" )

# Now generate and add in the "no ridge" specs
syn_classic <- augsynth(form = cigsalepcap ~ treated | retprice + xxincome + K1_r_15_24,
                        unit = state,
                        time = year,
                        data = tobacco_70, progfunc = "None" )
all_results_classic <- get_all_inference( syn_classic )

syn_classic_restr <- update_augsynth( syn_classic, drop = 2 )
all_results_classic_restr <- get_all_inference( syn_classic_restr )

tt2 = bind_rows( Tobacco = all_results_classic,
                 Tobacco_restr = all_results_classic_restr,
                 Kansas = all_results_k_classic, 
                 Kansas_restr = all_results_k_classic_restr,
                 .id = "data" )

# Stack ridge and no ridge and drop NAs
tt_all = bind_rows( classic=tt2, ridge=tt, .id = "adjustment" ) %>%
    mutate( width = upper_bound - lower_bound ) %>%
    filter( !is.na( width ) ) %>%
    separate( data, into = c("data", "restricted"), sep = "_", remove = FALSE ) %>%
    mutate(restricted = ifelse( is.na(restricted), "Full", "Restricted" ) )

#head(tt_all)

```

We get the following (Kansas on left, Tobacco on right)

```{r final_SE, echo=FALSE, eval=TRUE, fig.show="hold", fig.align='default', fig.height=3.5, fig.width=3}
ggplot( tt_all %>% filter( data != "Tobacco" ), aes( Time, width, col=name ) ) +
    facet_grid( adjustment ~ restricted ) +
    geom_line( size = 1) +
    theme_minimal() +
    geom_hline( yintercept = 0 ) +
    theme( legend.position = "none" ) +
    scale_x_continuous(labels = function(x) substr(x, 3, 4))  # Last two digits

ggplot( tt_all %>% filter( data == "Tobacco" ), aes( Time, width, col=name ) ) +
    facet_grid( adjustment ~ restricted ) +
    geom_line( size = 1) +
    theme_minimal() +
    geom_hline( yintercept = 0 ) +
    labs( color = "Inf: " ) +
    theme( legend.position = "none" ) +
    scale_x_continuous(labels = function(x) substr(x, 3, 4))  # Last two digits
```

Across our scenarios, the permutation approach tends to be most conservative, and for Kansas, has an unclear trend for extrapolation.
Across our examples, the R-statistic does stabalize over the raw permutation, giving tighter inference.
Restricting the donor pool stabalizes even further in the case of Kansas, and maybe does a little for Tobacco.
For the R-statistic approach, we see a steady climb in the interval width as we extrapolate further and further; I like this aspect of this approach.

The conformal widens under ridge adjustment for Kansas, but does not for the rest of the specifications.
It is also interesting how it is quite high for Tobacco and low for Kansas; it is driven by differnet structures of the data than the permutation approaches.

The Jackknife+ gives the tightest intervals, and they do not widen; I find myself suspicious of this approach given this behavior as compared to the other approaches.
It seems overly aggressive, as compared to the other forms of uncertainty quantification.

It would be interesting to see how these behaviors and trends were similar or different over even more datasets.
The `augsynth` package makes such explorations easy to do, and so we leave this for you.



# Acknowledgements

Thanks to the original designers of the `augsynth` package, in particular Eli Ben-Michael, for the support on the extensions used in this blog post.
Also thanks to Abadie for providing the data and original source code for the Synth package for the tobacco example.
This work was supported by the U.S. Department of Education, Institute for Education Sciences, Grant R305D200010. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education. 

