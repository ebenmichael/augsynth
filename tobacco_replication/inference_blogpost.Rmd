---
title: Comparing inference approaches for the synthetic control method with the `augsynth`
  package
author: "Miratrix & Delaney"
date: "2024-11-16"
output:
  pdf_document: default
  html_document: default
subtitle: Also replicating Abadie et al. (2010)
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(
    collapse = FALSE,
    message = FALSE,
    warning = FALSE,
    fig.align = 'center',
    comment = "#>"
)

## Install Synth if not already installed
# install.packages("Synth")

library(kableExtra)
library(magrittr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(tidyr)
library(rlang)

library(foreign)
library(graphics)
library(Synth)
library(augsynth)

## For the Abadie plots
replication_theme <- theme_bw(base_family = "Times New Roman") + 
    theme(panel.grid = element_blank(), 
          legend.background = element_rect(color = 'black'),
          legend.spacing.y = unit(0.0, 'pt'),
          legend.box.margin = margin(t = 5, r = 5, unit = 'pt'),
          legend.position = c(0.81, 0.9))

set.seed(1234)
```


There are many competing approaches to inference in the Synthetic Control world, and these approaches rely on a variety of different sets of assumptions that researchers can get quite agitated about.
In this post, we are going to compare some of these approaches by re-analyzing the famous "Tobacco example" originally laid out in [Abadie et al. (2010)](https://web.stanford.edu/~jhain/Paper/JASA2010.pdf) using some cool new features we recently added to the `augsynth` package.
This post also shows off our `augsynth` package, and in particular demonstrates how fitting and exploring synthetic control models can be easily done with only a few lines of code.

As a tiny bit of background, the synthetic control approach has two major steps.
The first step is to generate the synthetic control.
Intuitively, this consists of using some sort of algorithm to reweight a set of possible "donor" units so that their weighted average "looks like" the target treated unit, usually with respect to the treated unit's pre-treatment outcome trajectory and possibly some other baseline covariates besides.
There are an ever-increasing variety of ways one might do this, which we will discuss a bit more below.
Once you have your synthetic control, you estimate treatment impacts as the difference in the treated outcome and the synthetic control outcome for each post-treatment time period.

With synthetic control and consequent treatment impacts in hand, the second step is to figure out how to conduct inference: in the first step we deliberately manufactured a "synthetic" unit to look like our treated unit, and now we want to know if the difference in their behaviors post-treatment could be due to random chance, or something structural like a treatment effect.
Inference is a thorny problem since there is no easily well defined model or source of randomness to be seen!


## A note on different packages and approaches

In terms of R packages for fitting a synthetic control, we focus on two contenders.
The first is the `Synth` package, which has the algorithm and approach outlined in the seminal Abadie et al. (2010).
The second is the newer `augsynth` package, that offers a variety of different methods for building a synthetic comparison, and in fact _does not_ offer the exact same fitting approach used in `Synth`.
Nevertheless, we can fairly closely mimic the spirit of the original `Synth` with `augsynth`, as measured by getting fairly close to the same results shown in Figures 2--7 in the 2010 paper.
The `augsynth` package also offers a wider array of inference approaches which, as of the latest update, includes the permutation approach advocated for by the original Synth papers.
Before diving into inference, we first discuss how Synth and augsynth are different in how they build the synthetic control.

To use the `Synth` package, you specify a set of features (including a subset of pre-treatment outcome measures) you want to match on, and a separate set of pre-treatment "test" outcomes to assess performance on, and then the package will find the best weighting of the match features that produces the closest matched synthetic control, as measured by the specified pre-treatment "test" outcomes.
The package does this with a nested optimization of, for a given set of variable weights, finding the besdt  weighting of the donor units, and then finding the best variable weights that gives the "best of the best" synthetic control.
This iterative procedure results in two sets of weights, the "V" weights, which weight the possible match variables, and the donor weights, which weight the donor units to give the final synthetic control.
This approach makes it easy to include both lagged outcomes *and* auxiliary covariates, as the V weights will essentially rescale covariates to optimize any tradeoffs between which covariates to prioritize when building a match.

The `augsynth` package, by contrast, directly finds unit weights to minimize the mean squared difference between the synthetic control and the treated unit across all pre-treatment outcome periods and any additional covariates.
The scaling of match covariates can be done as a pre-processing step, but is not done within the package.
Instead of focusing on V weights, the `augsynth` package then allows for balance correction via a (usually ridge) regression step.
This second step is the "augmentation" part, and it can help regularize weights so, in the case of rich donor unit contexts, more donor units can be included in the synthetic match.
That said, the `augsynth` package can also just match on pre-treatment trends and not do any further adjustment, which is closer to the original `Synth` approach (but without being able to up- or down-weight some covariates or pre-treatment outcomes over others).

A more careful replication of the original Synth analysis with augsynth can be found in the new vignette in the package.

<!--We primarily focus on the 
The Rmd version of this notebook includes `Synth` code to generate Abadie et al.'s results with a high degree of fidelity (with the resulting `Synth`-based plots shown adjacent to analogous output from `augsynth` models).
-->


```{r import tobacco data, include=FALSE}
# Get working data CA and 38 control states 
Wk.data <- read.dta(here::here("./tobacco_replication/data/smoking_wkdata_39.dta"))

tobacco <- Wk.data %>%
    mutate(treated = ifelse((name == 'California') &
                                (year > 1988), 1, 0),
           state = name, # rename something meaningful
           state_index = index) %>%
    select(cigsalepcap, treated, state, year, retprice, xxincome, K1_r_15_24, xxbeer)

tobacco_70 <- tobacco %>% filter(year >= 1970) # subset data to 1970 forward to avoid issues due to missingess of outcomes
```


## Obtaining our synthetic control

The `augsynth` package divides the two steps into two function calls.
To get a synthetic control, we call `augsynth()`. 
To get inference, we call `summary()`.

Here we do our first step of getting our synthetic control.

```{r run tobacco model using augsynth}
syn <- augsynth(form = cigsalepcap ~ treated | retprice + xxincome + K1_r_15_24,
                unit = state,
                time = year,
                data = tobacco_70 )
syn
```

The above default `augsynth()` call removes bias via a ridge regression adjustment on top of calculating the synthetic control weights.
You can prevent this, making the fitting procedure closer in spirit to the Abadie approach, by including `progfunc = "none"`.
However, in this case, dropping the bias removal gives results that deviate more strongly from the tobacco paper.
We hypothesize that the bias removal is serving the role of the Abadie approach's variable importance weights (the bias removal being a regression that will naturally weight some variables more than others).
In augsynth, without a bias removal step, all the pre-treatment outcomes and covariates are weighted equally (after standardization).

We can plot our result with `plot(plot_type = 'outcomes')`: 

```{r  fig.width=4.5, fig.height=4}
p <- plot(syn, plot_type = 'outcomes raw average') + 
    ggtitle("Replication of Abadie et al. (2010), Figure 2")
p
```

If we do not want the raw average as a comparison line, simply use "outcomes" rather than "outcomes raw average."


## Getting our inference

We get our inference via the `summary()` method, specifying what inference approach to use:

```{r}
syn_rstat <- summary( syn, inf_type = 'permutation_rstat' )
syn_rstat
```

The inference options include those initially included in the package (`conformal`, `jackknife`, `jackknife+`), and now two that follow Abadie's permutation approach (`permutation` and `permutation_rstat`).
These last two refit the syn model to each donor unit in turn, and use the collection of pseudo-impact estimates as a reference distribution to compare our observed treatment trajectory to.
The `permutation` method uses the raw impact estimates, and `permutation_rstat` is the more recent approach promoted by Abadie of standardizing the gaps by the pre-treatment root mean squared predictive error (RMSPE).

We can plot and compare the confidence bands generated by our four different inference types (the "Jackknife" currently does not provide standard errors, so we omit it here):

```{r plot augsynth models, echo=FALSE, cache=TRUE, fig.width=8, fig.height=4.5}
all_results <- bind_rows(
    Conformal = summary(syn, inf_type = 'conformal')$att, 
    #Jackknife = summary(syn, inf_type = 'jackknife')$att, 
    `Jackknife+` = summary(syn, inf_type = 'jackknife+')$att, 
    Permutation = summary(syn, inf_type = 'permutation')$att, 
    `Permutation (rstat)` = syn_rstat$att,
    .id = 'name') %>% 
    mutate(name = factor(name, 
                         levels = c('Conformal', 'Jackknife', 'Jackknife+',
                                    "Permutation", "Permutation (rstat)"), 
                         ordered = T))

ggplot(all_results, aes(x = Time, y = Estimate, color = name)) + 
    geom_hline(yintercept = 0, linetype = 'solid') + 
    geom_vline(xintercept = syn$t_int, linetype = 'dashed') + 
    geom_line() +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = name), 
                alpha = 0.2, size = 0.1) + 
    facet_wrap(. ~ name) + 
    scale_x_continuous(breaks = seq(1970, 2010, 5)) + 
    labs(x = 'Year', y = 'Gap in per-capita cigarette sales\n(in packs)', 
         caption = 'ND replication: augsynth package') +
    scale_color_manual(values = c('red', 'blue', 'darkgreen', 
                                  'purple', 'orange'), 
                       guide = 'none', aesthetics = c('color', 'fill')) +
    theme_bw() +
    theme(plot.caption = element_text(hjust = 0, color = 'darkseagreen', 
                                      size = 9))
```

There are massive differences!
Conformal and Jackknife+ do not have widening uncertainty as time progresses---this is due to their assumption of, in effect, temporal exchangability.
In my mind this is a nonsensical assumption, and it leads to nonsensical results when extrapolating beyond the time of treatment.
The Jackknife+ seems to be extremely optimistic in terms of its inference, and does not pass face validity, in my mind.
The permutation approaches are more narrow towards the onset of treatment, and get wider as the placebo trajectories diverge--this seems sensible.
Interestingly, the width of these is on the order of the Conformal inference when you average across the span of time.
The RStat approach gives narrower intervals, likely by reducing the effect of outlier donor units that are hard to fit pre-treatment and diverge more post-treatment.
A tension here is whether we are standardizing our test statistic (like a studentized statistic for permutation testing) or artificially discounting the chance that a given unit may diverge markedly from the group.

The critique against permutation inference is a good one: our reference distribution of placebo trajectories is useful only if we think it is reasonable to believe the uncertainty of our treated unit's trajectory would be similar.
If our treated unit is different from the donor pool (which it likely is---it was the unit that got treatment, after all) then the reference distribution may not a good reference.
For example, if the treatment unit is harder to build a good synthetic control for than the donor units, then how the donor unit trajectories diverge from 0 after the time of treatment may not capture the true randomness (whatever that is) of the treatment unit at all.

All of this said, the reference distribution does give us _some_ sense of what this kind of exercise is likely to do for the kind of data we are looking at.
And, given the other options, it seems much better than any provided alternative.



## Where do the permutation confidence intervals come from?

The confidence regions for the permutation approaches come from measuring the uncertainty of the placebo distribution at each time point.
We can look at the raw placebo trajectories as so:

```{r fig.width=4.5, fig.height=4}
plot(syn_rstat, 
     plot_type = 'placebo',
     inf_type = 'permutation',
) + ggtitle("Replication of Abadie et al. (2010), Figure 4")
```

We then use the standard deviation of the trajectories to estimate a rough "Standard Error."
We can also get a table of the placebo trajectories via the `placebo_distribution()` method:

```{r show_perm_inference}
placebo_distribution( syn_rstat )
```

# Examining and curating the donor units

As a somewhat more appendix-like Appendix, in the sense of the vestigal organ bearing the same name, we close with a few other things you can do with the `augsynth` package to manage your pool of donor units.

First, the `donor_table()` function will return a summary dataframe with the RMSPE and synthetic weight for each of the donor units.  For example, when not using the ridge regression bias adjustment, we typically get sparse weights on our units, and might want to see which units have non-zero:

```{r}
syn_SCM <- augsynth(form = cigsalepcap ~ treated | retprice + xxincome + K1_r_15_24,
                    unit = state,
                    time = year,
                    data = tobacco_70, progfunc = "none" )

donor_table( syn_SCM ) %>%
    dplyr::filter( weight > 0 ) %>%
    arrange( -abs(weight) )
```

Second, we can easily exclude certain donor units and automatically recalculate our synthetic control via the `update_augsynth()` method.
We specify which donor units to drop via the `drop` argument.
If `drop` is passed as a numeric, then all donor units with an RMSPE greater than `drop` times the treated unit's pre-treatment RMSPE will be dropped. For example, setting `drop = 2` will exclude any units with a pre-treatment RMSPE twice as large as California's.
The updated augsynth model inherits the inference type, formula, and data structure of the original model.

Here we replicate Figures 5, 6, and 7 from Abadie et al. (2010) by dropping donor units with pre-treatment RMSPE greater than 20, 5, and 2 times California's pre-treatment RMSPE, respectively:

```{r Figures 6 and 7, echo=TRUE, eval=TRUE, fig.show="hold", fig.align='default', fig.height=4.5, fig.width=4.6}
update_augsynth(syn) %>% # drops units with >20x treated RMSPE by default
    summary( inf_type = 'permutation' ) %>%
    plot(plot_type = 'placebo' ) + 
    ggtitle("Replication of Abadie et al. (2010), Figure 5") + ylim(-51, 91) + 
    annotate('text', y = -48, x = 1970, 
             label = "Removes donors with 20x California's  pre-treatment RMSPE",
             hjust = 0, color = 'darkseagreen', size = 3) 
update_augsynth(syn, drop = 5) %>% 
    summary( inf_type = 'permutation' ) %>%
    plot(plot_type = 'placebo') + 
    ggtitle("Replication of Abadie et al. (2010), Figure 6") + ylim(-51, 91) + 
    annotate('text', y = -48, x = 1970, 
             label = "Removes donors with 5x California's pre-treatment RMSPE",
             hjust = 0, color = 'darkseagreen', size = 3) 
update_augsynth(syn, drop = 2) %>% 
    summary( inf_type = 'permutation' ) %>%
    plot(plot_type = 'placebo') + 
    ggtitle("Replication of Abadie et al. (2010), Figure 7") + ylim(-51, 91) + 
    annotate('text', y = -48, x = 1970, 
             label = "Removes donors with 2x California's pre-treatment  RMSPE",
             hjust = 0, color = 'darkseagreen', size = 3) 
```

We can instead drop donor units by using their unit identifiers as follows:

```{r permutation without states, echo=TRUE, eval=TRUE, fig.show="hold", fig.align='default', fig.height=4.5, fig.width=4.6}
drop_states <- c("Iowa", "Arizona", "Alabama", "Illinois", "Indiana", 
                 "Idaho", "Connecticut", "New Mexico", "Texas", "Utah", 
                 "North Dakota", "South Dakota", "Vermont", "Wisconsin", 
                 "West Virginia", "Wyoming", "Tennessee", "Pennsylvania")

update_augsynth(syn, drop = drop_states) %>% 
    summary( inf_type = 'permutation' ) %>%
    plot(plot_type = 'placebo') 
```


# Acknowledgements

Thanks to the original designers of the augsynth package, in particular Eli Ben-Michael, for the support on these extensions.
Also thanks to Abadie for providing the data and original source code for the Synth package.
This work was supported by the U.S. Department of Education, Institute for Education Sciences, Grant R305D200010. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education. 

